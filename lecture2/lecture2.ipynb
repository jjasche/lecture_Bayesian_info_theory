{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "## Bayesian information theory: Lecture 2\n",
    "\n",
    "*This jupyter notebook is part of a [collection of notebooks](../index.ipynb) on various topics of Bayesian Information Theory. Please direct questions and suggestions to [j.jasche@tum.de](mailto:j.jasche@tum.de).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem\n",
    "The origin of the Bayesian philosophy lies in an interpretation of Bayes' theorem. <br/><br/>\n",
    "Her we discuss the derivation and application of the theorem.\n",
    "\n",
    "## Bayes' law\n",
    "Bayes's law is given by: <br/><br/>\n",
    "\n",
    "<center> $P(A|B,X)=P(A|X)\\,\\frac{P(B|A,X)}{P(B|X)}$ </center>\n",
    "\n",
    "We then call:\n",
    "<blockquote>\n",
    "<p> 1) $P(A|B,X)$ the posterior of $A$ given $B$ and some background information $X$.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 2) $P(A|X)$ the prior of $A$ in light of some background information $X$.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 3) $P(B|A,X)$ the likelihood of the observation $B$ given a proposition $A$ and some background information $X$\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 4) $P(B|X)$ the normalization or evidence\n",
    "</blockquote>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Major criticism: The prior!\n",
    "\n",
    "\n",
    "<p>** Def 5 **: The prior</p>\n",
    "<blockquote>\n",
    "<p> The prior is the probability that expresses our degree of belief or plausibility about a proposition $A$ before some evidence is taken into account.</p>\n",
    "</blockquote>\n",
    "\n",
    "## Is the posterior a probability\n",
    "\n",
    "The posterior can only be accepted as our updated degree of plausibility or belief if agrees with the laws of probability derived in lecture 1.\n",
    "\n",
    "** Check sum rule: ** <br/><br/>\n",
    "\n",
    "<center> $\\begin{eqnarray} 1 &=& P(A|B,X) + P(\\overline{A}|B,X) \\nonumber \\\\ &=& \\frac{P(A|X)P(B|A,X)}{P(B|X)} + \\frac{P(\\overline{A}|X)P(B|\\overline{A},X)}{P(B|X)} \\nonumber \\\\   \\end{eqnarray}$ </center>\n",
    "\n",
    "holds for existing P(B|X) and:\n",
    "\n",
    "<center> $0 < P(B|X)=P(A|X)P(B|A,X) + P(\\overline{A}|X)P(B|\\overline{A},X)$ </center><br/><br/>\n",
    "\n",
    "the posterior solves the sum rule\n",
    "\n",
    "** Check product rule: ** <br/><br/>\n",
    "The proof is trivial. For existing $P(B|X)>0$ the posterior fulfills the product by construction.\n",
    "\n",
    "\n",
    "** Check $0\\leq P(A|B,X) \\leq 1$ : ** <br/><br/>\n",
    "For existing $P(B|X)>0$ : <br/><br/>\n",
    "\n",
    "<center> $ 0\\leq P(A,B|X) \\leq 1 $ </center><br/><br/>\n",
    "\n",
    "Division by $P(B|X)$ yields:<br/><br/>\n",
    "\n",
    "<center> $ \\begin{eqnarray} 0 \\leq &\\frac{P(A,B|X)}{P(B|X)}& \\leq \\frac{1}{P(B|X)} \\nonumber \\\\ 0\\leq &P(A|B,X)& \\leq \\frac{1}{P(B|X)} \\end{eqnarray}$ </center> <br/><br/>\n",
    "\n",
    "Thus $0\\leq P(A|B,X)$. Analogously we obtain $0\\leq P(\\overline{A}|B,X)$. Then: <br/><br/> \n",
    "\n",
    "<center> $ P(A,B|X) = 1 - P(\\overline{A}|B,X)  \\leq 1 $ </center><br/><br/>\n",
    "\n",
    "Since the sum rule holds and $0\\leq P(\\overline{A}|B,X)$ the statement is proven. <br/><br/>\n",
    "\n",
    "\n",
    "Therefore posterior is a probability and thus a measure of plausibility if and only if:\n",
    "\n",
    "1) $P(A,B|X)$ exists and is a probability. <br/><br/>\n",
    "2) $P(B|X)=P(A|X)P(B|A,X) + P(\\overline{A}|X)P(B|\\overline{A},X)>0$. <br/><br/>\n",
    "\n",
    "\n",
    "## Example: Bayesian reasoning\n",
    "\n",
    "Example taken from:\n",
    "\n",
    "* Kevin B. Korb, Ann E. Nicholson, *Bayesian Artificial Intelligence, Second Edition*, Chapman & Hall/CRC, 2010 <br/><br/>\n",
    "\n",
    "Suppose the women attending a particular clinic show a long-term chance of 1 in 100 of having breast cancer. Suppose also that the initial screening test used at the clinic has a false positive rate of 0.2 (that is, 20 percent of women without cancer will test positive for cancer) and that it has a false negative rate of 0.1 ( that is 10 percent of women with cancer will test negative). The laws of probability dictate from these facts that the probability of a positive test given cancer is 90 percent. Suppose you have been tested positive, what is the probability of having cancer?\n",
    "\n",
    "** Probability of a positive test ** <br/><br/>\n",
    "So clearly the probability of testing positive given that a patient has cancer is:\n",
    "$\\begin{eqnarray}\n",
    "P(Pos| cancer) &=& 0.9 \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "** What is the chance of having cancer **<br/><br/>\n",
    "But this does not answer the question most of us would like to have answered and that is what is the probability of having cancer given a positive test. Note that this is not the same question. In analogy to propositional calculus\n",
    "a positive test does not equal to a patient having cancer.  \n",
    "\n",
    "$\\begin{eqnarray}\n",
    "P(Cancer| Pos) &=& \\frac{P(Pos | Cancer) P(Cancer)}{P(Pos)} \\nonumber \\\\\n",
    "&=& \\frac{P(Pos | Cancer) P(Cancer)}{P(Pos|Cancer) P(Cancer) + P(Pos|\\overline{Cancer}) P(\\overline{Cancer})} \\nonumber \\\\\n",
    "&=& \\frac{0.9 \\times 0.01}{0.9 \\times 0.01 + 0.2 \\times 0.99}\\nonumber \\\\\n",
    "&\\approx& 0.043 \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "Now the discrepancy between 4 percent and 80 or 90 percent is no small matter particularly if the consequences of an error involves unnecessary surgery or (in the reverse case) leaving a cancer untreated. \n",
    "\n",
    "\n",
    "## Bayesian learning\n",
    "\n",
    "Assume we have observations $B_1,B_2$ then frequen application of Bayes's law yields:\n",
    "\n",
    "<center> $ \\begin{eqnarray} P(A|B_1,B_2,X) &=& \\frac{ P(A|X)\\,P(B_1,B_2|A,X)}{P(B_1,B_2|X)} \\nonumber \\\\ &=&\\frac{ P(A|X)\\,P(B_1|A,X)}{P(B_1|X)} \\frac{\\,P(B_2|B_1,A,X)}{P(B_2|B_1,X)} \\nonumber \\\\ &=& P(A|B_1,X) \\frac{\\,P(B_2|B_1,A,X)}{P(B_2|B_1,X)} \\nonumber\\end{eqnarray}$ </center> <br/><br/>\n",
    "\n",
    "Where in the last line we see, that the new posterior $P(A|B_1,B_2,X)$ is just the previous posterior $P(A|B_1,X)$ multiplied with the likelihood of the new observation $B_2$. Bayesian learning is therefore the sequential updating of the posterior distribution in light of new observations. <br/><br/>\n",
    "\n",
    "A generalization to arbitrary many observations $B_N$ is left as a homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Bayes rule to many propositions\n",
    "\n",
    "<p>** Def 6 **: Exhaustive sets</p>\n",
    "<blockquote>\n",
    "<p> We call the set of propositions or hypotheses $A_1,A_2, ... , A_N$ an collectively exhaustive set if at least one is true. We further require that the prior sums to one over all propositions:  <br/><br/>\n",
    "  <center> $ \\sum_i P(A_i|X) = 1$ </center> </p>\n",
    "</blockquote>\n",
    "\n",
    "Followin the laws of probability the posterior is then given by:  <br/><br/>\n",
    "\n",
    "<center> $P(A_i|B,X) = \\frac{P(A_i|X) P(B|A_i,X)}{\\sum_j P(A_j|X)P(B|A_j,X)}$ </center> <br/><br/>\n",
    "\n",
    "Some comments are required. The set of propositions $A_1,A_2,...,A_N$ needs to be an exhaustive set. Suppose non of the $A_i$ would be an explanation of $B$. Then $P(B|X)$ would not exist not allowing for conditionalization and Bayesian reasoning. One can always create an exhaustive set by adding the negation $A_{N+1}=\\overline{A_1+A_2+...+A_N}$. Therefore always consider the possibility that there may exist solutions that you have not included into your list of possible explanations. \n",
    "\n",
    "\n",
    "## Bayesian Hypothesis testing\n",
    "\n",
    "Philosophy of science: <br/><br/>\n",
    "\n",
    "<center> \"We do not rule out models, we just determine their relative plausibilities.\" </center> <br/><br/>\n",
    "\n",
    "Note that this is a direct instruction to design hypotheses tests in a Bayesian scenario. The posterior quantifies the degree of plausibility or belief that a proposition $A_1$ is true. We may now wonder if proposition $A_1$ is more plausible than a second propositione $A_2$. To do this we just estimate the ratios of the two posteriors:\n",
    "\n",
    "<center> $\\begin{eqnarray}\\frac{P(A_1|B,X)}{P(A_2|B,X)} &=& \\frac{P(A_1|X)\\,P(B|A_1,X)}{P(A_2|X)\\,P(B|A_2,X)}\\frac{\\sum_j P(A_j|X)P(B|A_j,X)}{\\sum_j P(A_j|X)P(B|A_j,X))}\\nonumber\\\\ &=& \\frac{P(A_1|X)\\,P(B|A_1,X)}{P(A_2|X)\\,P(B|A_2,X)} \\end{eqnarray}$ </center> <br/><br/>\n",
    "\n",
    "We call:\n",
    "\n",
    "<center> $O(A_1,A_2|X)=\\frac{P(A_1|X)}{P(A_2|X)} $</center><br/><br/>\n",
    "\n",
    "the prior odds and:\n",
    "\n",
    "<center> $O(A_1,A_2|B,X)=\\frac{P(A_1|B,X)}{P(A_2|B,X)} $</center><br/><br/>\n",
    "    \n",
    "the posterior odds.\n",
    "\n",
    "Then we can write the ratio of the posterior and the prior odds as:\n",
    "\n",
    "<center> $\\frac{O(A_1,A_2|B,X)}{O(A_1,A_2|X)}=\\frac{P(B|A_1,X)}{P(B|A_2,X)} = BF_{1,2} $</center><br/><br/>\n",
    "    \n",
    "Where we call $BF_{1,2}$ the Bayes factor. It is clear that Bayes' factor in favour of $A_1$ will only rise if and only if the likelihood ratio is greater than one.\n",
    "\n",
    "### Some Advantages of Bayesian Hypothesis testing\n",
    "\n",
    "Bayes factors and posterior model probabilities describe the relative support or plausibility for a set of candidate hypotheses. Other advantages of Bayesian hypothesis testing include the following:\n",
    "\n",
    "** 2) Evidence can be obtained in favour of the Null hypothesis **\n",
    "\n",
    "Bayesian hypothesis testing allows one to obtain evidence in favor of the null hypothesis. Because theories and models often predict the absence of a difference, it is vital for scientific progress to be able to quantify evidence in favor of the null hypothesis.\n",
    "\n",
    "** 1) guaranteed coherence **\n",
    "\n",
    "Suppose we have a set of three candidate hypotheses, $A_1$,$A_2$ and $A_3$. Then:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac{P(A_1|B,X)}{P(A_3|B,X)} = \\frac{P(A_1|B,X)}{P(A_2|B,X)}\\, \\frac{P(A_2|B,X)}{P(A_3|B,X)}  \n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "In terms of Bayes factors this reads:\n",
    "\n",
    "$\\begin{equation}\n",
    "BF_{1,3} = BF_{1,2}\\, BF_{2,3} \n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "For instance, when the data are five times as likely to occur under $A_1$ than under $A_2$, and seven time as likely under $A_2$ than under $A_3$, it follows that the data are $5 \\times 7 = 35$ times as likely under $A_1$ than under $A_3$. \n",
    "\n",
    "This is a genuine Bayesian result and no comparable result exists in classical statistics.\n",
    "\n",
    "### Interpretation of Bayes factors: Jeffreys scale\n",
    "\n",
    "To interpret these values Jeffreys introduced the following scale scale\n",
    "\n",
    "\n",
    "| $BF$   | Strength of evidence    | \n",
    "| ----------------------- |:-----------------------:| \n",
    "| $10^0$ to $10^{1/2}$    | barely worth mentioning |\n",
    "| $10^{1/2}$ to $10^{1}$  | substantial             |\n",
    "| $10^1$ to $10^{3/2}$    | strong                  |\n",
    "| $10^{3/2}$ to $10^{2}$  | very strong             |\n",
    "| $>10^{2}$               | decisive                |\n",
    "\n",
    "\n",
    "Example: Breast cancer\n",
    "\n",
    "\n",
    "1) $\\frac{P(Cancer| Pos)}{1-P(Cancer| Pos)} \\approx 0.045$ \n",
    "\n",
    "The Bayes factor dos not favour the hypothesis of the patient having cancer over the ptient having no cancer.  \n",
    "\n",
    "\n",
    "2) $\\frac{1-P(Cancer| Pos)}{P(Cancer| Pos)} \\approx 22.26$ \n",
    "\n",
    "There is strong evidence in favour of the hypothesis that the patient has no cancer.\n",
    "\n",
    "Note that both tests are not decisive. Additional tests can increase the certainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Variables\n",
    "\n",
    "<p>** Def 7 **: </p>\n",
    "<blockquote>\n",
    "<p> A random variable  $Y$ is a variable which reports the outcome of an experiment. The random variable $Y$ can take different \n",
    "states $y_i$, which we write as:\n",
    "    <center> $ Y = y_i$ </center> \n",
    "The set of states a variable $Y$ can take form its state space $\\Omega_Y$,and its size is $\\left|\\Omega_Y\\right|$ </p>\n",
    "</blockquote>\n",
    "\n",
    "<p>** Def 8 **: </p>\n",
    "<blockquote>\n",
    "<p> A density function $f(Y)$ assigns a weight to each possible value of the continous random variable $Y$.\n",
    "    $f(\\cdot)$ is no probability, but it can generate a probability if:\n",
    "    <blockquote>\n",
    "    <p>1) $f(y) \\geq 0$  for all $y \\in \\Omega_Y$</p>\n",
    "    </blockquote> \n",
    "    <blockquote>\n",
    "    <p>2) $\\int_{\\Omega_Y}f(y) \\mathrm{d}y = 1$</p>\n",
    "    </blockquote> \n",
    "    We call this a bonafide distribution. </p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<p>** Def 9 **: </p>\n",
    "<blockquote>\n",
    "<p> The cumulative distribution function is defined as:\n",
    "     <center> $ F(y) = P(Y\\leq y) = \\int_{y'\\leq y}f(y') \\mathrm{d}y'$ </center> \n",
    "    Similarly:\n",
    "    <center> $ F(a\\leq y \\leq b) = P(a\\leq Y\\leq b) = \\int_{a\\leq y'\\leq b}f(y') \\mathrm{d}y'$ </center> </p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "# Bayes law with continuous variables\n",
    "\n",
    "For a discrete state-space Bayes theorems readily hold. We can just map the discrete state-space $\\Omega_Y$ to the discrete event-space $A_1,A_2,...,A_N$. For continuous variables one can map infinitesimal intervas to th event-space. In the limit we obtain Bayes law for distributions. Note, that a specific parametrization is always associated to the choice of model $M$. In the following we will explicitly state the dependence on the model $M$. Bayes law for distributions is then given as:\n",
    "\n",
    "  <center> $ \\pi(y|d,M)= \\frac{\\pi(y|M)\\,\\pi(d|y,M)}{\\int \\pi(y'|M)\\,\\pi(d|y',M) \\mathrm{d}y' }$</center> \n",
    "  \n",
    "  \n",
    "With:\n",
    "\n",
    "<blockquote>\n",
    "<p> 1) $\\pi(y|d,M)$ : the posterior distribution of $y$ given the data $d$ and a model $M$.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 2) $\\pi(y|M)$ : prior distribution of $y$ given the model $M$.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 3) $ \\pi(d|y,M)$ : likelihood distribution of the data $d$ given $y$ and a model $M$.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 4) $\\int \\pi(y'|M)\\,\\pi(d|y',M) \\mathrm{d}y' $ the normalization or evidence\n",
    "</blockquote>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# Some probability calculus\n",
    "\n",
    "<p>** Def 10 **: Independence </p>\n",
    "<blockquote>\n",
    "<p> We say a variable $Y$ is independent of a variable $Z$ if the joint distribution factorizes:\n",
    "     <center> $ \\pi(y,z) = \\pi(y) \\pi(z|y) = \\pi(y)\\pi(z)$ </center> </p>\n",
    "</blockquote>\n",
    "\n",
    "<p>** Def 11 **: Conditional independence </p>\n",
    "<blockquote>\n",
    "<p> We say a variable $Y$ is conditionally independent of a variable $Z$ giveb some information $w$ if:\n",
    "     <center> $ \\pi(y,z|w) = \\pi(y|w) \\pi(z|y,w) = \\pi(y|w)\\pi(z|w) $</center> </p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<p>** Def 12 **: Marginalization </p>\n",
    "<blockquote>\n",
    "<p> Let $\\pi(y,z|d,M)$ be the joint posterior distribution of $y$ and $z$ given data $d$ and model $M$. Then we call:\n",
    "     <center> $ \\pi(y|d,M) = \\int  \\pi(y,z|d,M) \\mathrm{d}z$</center>\n",
    "    the marginalized posterior distribution.</p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "# Bayesian hypothesis testing with parameterized models:\n",
    "\n",
    "Again we need to evaluate relative model plausibility. How do we calculate the plausibility of parameterized models?\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(M|d) &=& \\int  \\pi(y,M|d) \\mathrm{d}y \\nonumber \\\\ &=& \\int \\pi(y,M) \\frac{\\pi(d|y,M)}{\\pi(d)} \\mathrm{d}y \\nonumber \\\\ &=& \\int \\pi(M) \\pi(y|M) \\frac{\\pi(d|y,M)}{\\pi(d)} \\mathrm{d}y \\nonumber\\\\  &=& \\frac{\\pi(M)}{\\pi(d)} \\int  \\pi(y|M) \\pi(d|y,M) \\mathrm{d}y \\nonumber\\\\ &=& \\frac{\\pi(M) \\pi(d|M)}{\\pi(d)}  \\end{eqnarray}$</center>\n",
    "\n",
    "\n",
    "with:\n",
    "\n",
    "<center> $ \\pi(d|M) =  \\int  \\pi(y|M) \\pi(d|y,M) \\mathrm{d}y $</center>\n",
    "\n",
    "and the Bayes factor is then given by:\n",
    "\n",
    "<center> $ BF_{1,2} = \\frac{\\int  \\pi(y|M_1) \\pi(d|y,M_1) \\mathrm{d}y}{\\int  \\pi(y|M_2) \\pi(d|y,M_2) \\mathrm{d}y}  $</center>\n",
    "\n",
    "Bayesian model comparison can easily be extended to an arbitrary number of parameters. It is just required to do the propper marginalization over all model parameters.\n",
    "\n",
    "\n",
    "# Bayesian parsimony\n",
    "<center> \"The explanation requiring the fewesr assumptions is most likely to be correct.\"  William of Ockham</center>\n",
    "\n",
    "\n",
    "Task: We want to compare models of different degrees of freedom.\n",
    "\n",
    "\n",
    "<p>** Def 13 **: Maximum a posteriori estimate </p>\n",
    "<blockquote>\n",
    "<p> We define the maximum a posteriori (MAP) estimate $y^*$ as a solution to:\n",
    "     <center> $ \\left . \\frac{\\partial \\mathrm{ln}(\\pi(y|d,M))}{\\partial _y} \\right | _{y=y^*} = 0$ </center>\n",
    "    This definition generalizes trivially to multivariate distributions.</p>\n",
    "</blockquote>\n",
    "\n",
    "The model evidence for multivariate models is given as:<br/><br/>\n",
    "\n",
    "\n",
    "<center> $ \\pi(d|M) =  \\int  \\pi(y'_1,y'_2,...,y'_N|M)\\, \\pi(d|y'_1,y'_2,...,y'_N,M) \\mathrm{d}y'_1\\,\\mathrm{d}y'_2\\,...\\,\\mathrm{d}y'_N $</center><br/><br/>\n",
    "\n",
    "We will use the Laplace approximation to approximate the integral around the MAP values $y^*_1,y^*_2,...,y^*_N$. To do that we will expand the logharithmic posterior distribution to second order around $y^*_1,y^*_2,...,y^*_N$:<br/><br/>\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\mathrm{ln}\\left(\\pi(y_1,y_2,...,y_N|M)\\, \\pi(d|y_1,y_2,...,y_N,M)\\right) &=& \\mathrm{ln}\\left(\\pi(y^*_1,y^*_2,...,y^*_N|M)\\, \\pi(d|y^*_1,y^*_2,...,y^*_N,M)\\right) \\nonumber \\\\ & & + \\left . \\sum_i \\frac{\\partial \\mathrm{ln}\\left(\\pi(y_1,y_2,...,y_N|M)\\, \\pi(d|y_1,y_2,...,y_N,M)\\right)}{\\partial y_i } \\right |_{y_i=y^*_i} (y_i - y^*_i) \\nonumber \\\\ & & + \\frac{1}{2} \\left . \\sum_i \\sum_j \\frac{\\partial^2 \\mathrm{ln}\\left(\\pi(y_1,y_2,...,y_N|M)\\, \\pi(d|y_1,y_2,...,y_N,M)\\right)}{\\partial y_i \\partial y_j } \\right |_{y_i=y^*_i,y_j=y^*_j} (y_i - y^*_i)\\,(y_j - y^*_j) \\nonumber \\\\ &=& \\mathrm{ln}\\left(\\pi(y^*_1,y^*_2,...,y^*_N|M)\\, \\pi(d|y^*_1,y^*_2,...,y^*_N,M)\\right) \\nonumber \\\\ & & +\\frac{1}{2} \\left . \\sum_i \\sum_j \\frac{\\partial^2 \\mathrm{ln}\\left(\\pi(y_1,y_2,...,y_N|M)\\, \\pi(d|y_1,y_2,...,y_N,M)\\right)}{\\partial y_i \\partial y_j } \\right |_{y_i=y^*_i,y_j=y^*_j} (y_i - y^*_i)\\,(y_j - y^*_j) \\nonumber \\\\ &=& \\mathrm{ln}\\left(\\pi(y^*_1,y^*_2,...,y^*_N|M)\\, \\pi(d|y^*_1,y^*_2,...,y^*_N,M)\\right) \\nonumber \\\\ & & -\\frac{1}{2}  \\sum_i \\sum_j D^{-1}_{ij} (y_i - y^*_i)\\,(y_j - y^*_j)  \\end{eqnarray}$ </center><br/><br/>\n",
    "\n",
    "where we introduced:<br/><br/>\n",
    "\n",
    "<center> $ \\begin{eqnarray} D^{-1}_{ij} = \\left | \\left .\\frac{\\partial^2 \\mathrm{ln}\\left(\\pi(y_1,y_2,...,y_N|M)\\, \\pi(d|y_1,y_2,...,y_N,M)\\right)}{\\partial y_i \\partial y_j } \\right |_{y_i=y^*_i,y_j=y^*_j} \\right | \\, , \\end{eqnarray}$ </center><br/><br/>\n",
    "since $y^*_1,y^*_2,...,y^*_N$ defines a maximum, and the curvature is negative at that point.\n",
    "\n",
    "We can then get:\n",
    "\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(d|M) \\approx &=& \\pi(y^*_1,y^*_2,...,y^*_N|M)\\, \\pi(d|y^*_1,y^*_2,...,y^*_N,M) \\int \\mathrm{exp}\\left ( -\\frac{1}{2}  \\sum_i \\sum_j D^{-1}_{ij} (y_i - y^*_i)\\,(y_j - y^*_j) \\right)  \\mathrm{d}y'_1\\,\\mathrm{d}y'_2\\,...\\,\\mathrm{d}y'_N \\nonumber \\\\ &=& \\pi(y^*_1,y^*_2,...,y^*_N|M)\\, \\pi(d|y^*_1,y^*_2,...,y^*_N,M) \\, \\sqrt{\\mathrm{det}(2\\pi D)}  \\end{eqnarray}$\n",
    "    \n",
    "    \n",
    "For the sake of demonstration assume a multivariate normal distribution with means $\\mu_1,\\mu_2, ... , \\mu_N $ and covariance matrix $S_{ij}$. Then we can write:\n",
    "\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(d|M) \\approx &=& \\mathrm{exp}\\left ( -\\frac{1}{2}  \\sum_i \\sum_j S^{-1}_{ij} (y^*_i - \\mu_i)\\,(y^*_j - \\mu_j) \\right)  \\pi(d|y^*_1,y^*_2,...,y^*_N,M) \\, \\sqrt{\\frac{\\mathrm{det}(2\\pi D)}{\\mathrm{det}(2\\pi S)}}  \\end{eqnarray}$\n",
    "\n",
    "The last factor is the Ockhams factor. It decides on the parsimony of the problem.\n",
    "\n",
    "To illustrate the fact we first look at a 1d problem. Then: \n",
    "\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(d|M) \\approx &=& \\mathrm{exp}\\left ( -\\frac{1}{2}  \\frac{\\left(y^* - \\mu\\right)^2}{\\sigma_S^2} \\right)  \\pi(d|y^*,M) \\, \\frac{\\sigma_D}{\\sigma_S}  \\end{eqnarray}$\n",
    "\n",
    "where $\\sigma_S$ and $\\sigma_D$ are the prior and posterior standard deviations, respectively.\n",
    "We will further assume $\\sigma_S \\gg \\sigma_D$ then the first factor in the above equation is quasi constant over the posterior domain and:  \n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(d|M) \\approx &=&   \\pi(d|y^*,M) \\, \\frac{\\sigma_D}{\\sigma_S}  \\end{eqnarray}$\n",
    "    \n",
    "Since the last factor $\\sigma_D/\\sigma_S<1$ this Ockhams factor gives a punishment for wasteful prior volume coverage.\n",
    "\n",
    "Now extent the argiment to $N$ dimensions. For the sake of the argument we assume both matrices $D$ and $S$ to be diagonal with corresponding diagonal elements $\\sigma_D$ and $\\sigma_S$ respectively. Evaluating the corresponding determinants then yields:\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(d|M) \\approx &=&   \\pi(d|y^*_1,y^*_2,...,y^*_N,M)  \\, \\left(\\frac{\\sigma_D}{\\sigma_S}\\right)^N  \\end{eqnarray}$\n",
    "    \n",
    "Since $\\sigma_D/\\sigma_S<1$ the Ockhams factor provides an exponential suppression with the number of dimensions $N$.\n",
    "Ockhams factor therefore punishes too complex models. \n",
    "\n",
    "In order to accept a more complex model as an explanation for the observations, the likelihood value of the best fit $\\pi(d|y^*_1,y^*_2,...,y^*_N,M) $ has to outweigh the Ockhams factor. However, for sufficiently large numbers of parameters $\\pi(d|y^*_1,y^*_2,...,y^*_N,M) $ will not change anymore if we add another parameter, since we already have a perfect fit to the observations. Latest at that point Ockhams factor will take over and punish models with higher number of parameters. \n",
    "\n",
    "Bayesian model testing therefore has a build-in Ockhams razor, and the model selection process automatically selects the model with the least number of parameters, required to explain the observations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
