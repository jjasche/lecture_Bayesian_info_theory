{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "## Bayesian information theory: Lecture 3\n",
    "\n",
    "*This jupyter notebook is part of a [collection of notebooks](../index.ipynb) on various topics of Bayesian Information Theory. Please direct questions and suggestions to [j.jasche@tum.de](mailto:j.jasche@tum.de).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Savage-Dickey test\n",
    "The Savage-Dickey method is an approach to estimate the Bayes factors for nested models (Dickey 1971).\n",
    "\n",
    "<p>** Def 13 **: Nested models</p>\n",
    "<blockquote>\n",
    "<p> A model $M_0$ is nested in model $M_1$ if the parameters of model $M_0$ are a subset of the parameters of model $M_1$.</p>\n",
    "</blockquote>\n",
    "\n",
    "Assume model $M_1$ is parameterized by a set of parameters $y_0,...y_N,z_0,...,z_M = \\vec{y},\\vec{z}$ (to simplify the notation we group parameters into vectors $\\vec{y}$ and $\\vec{z}$ ). Then we obtain a nested model $M_o$ by fixing a subset of the model parameters of model $M_1$ to some fixed values $\\vec{z}=\\vec{z}_0$. We can then evaluate the Bayes factors between model $M_0$ and $M_1$ as follows: <br/><br/>\n",
    "\n",
    "<center> $BF_{0,1} = \\frac{\\pi(d|M_0)}{\\pi(d|M_1)} = \\frac{\\pi(d|\\vec{z}=\\vec{z}_0,M_1)}{\\pi(d|M_0)} $ </center> <br/><br/>\n",
    "\n",
    "Now we will use Bayes' law to evaluate the numerator:<br/><br/>\n",
    "\n",
    "<center> $\\pi(d|\\vec{z}=\\vec{z}_0,M_1) = \\frac{\\pi(d|M_1)\\,\\pi(\\vec{z}=\\vec{z}_0|d,M_1)}{\\pi(\\vec{z}=\\vec{z}_0|M_1)} $ </center> <br/><br/>\n",
    "\n",
    "Thus the Bayes factor can be written as:<br/><br/>\n",
    "\n",
    "\n",
    "<center> $BF_{0,1} = \\frac{\\pi(\\vec{z}=\\vec{z}_0|d,M_1)}{\\pi(\\vec{z}=\\vec{z}_0|M_1)}  $ </center> <br/><br/>\n",
    "\n",
    "\n",
    "For nested models the Bayes factor is just the fractional change from the prior to the posterior in the marginal density of $\\vec{z}$ at $\\vec{z}=\\vec{z}_0$.\n",
    "\n",
    "\n",
    "# Posterior predictive test\n",
    "\n",
    "The Bayes factor will always provide us with the relative plausibility between models, no matter of the quality of the models. But do the inferences from the model make sense? At least during model building we should therefore always check whether the model can actually predict the observations. This can be achieved by generating predicted data $d^*$ and compare predicted observations $d^*$ to actual observations $d$. Such tests are called posterior predictive tests and new data is generated as follows:<br/><br/>\n",
    "\n",
    "<center> $\\begin{eqnarray}\\pi(d^*|d,M) &=& \\int \\mathrm{d}y \\pi(y,d^*|d,M) \\nonumber \\\\ &=& \\int \\mathrm{d}y \\pi(y|d,M) \\pi(d*|y,d,M)\\nonumber \\\\ &=& \\int \\mathrm{d}y \\pi(y|d,M) \\pi(d*|y,M) \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "We can then do the following checks:\n",
    "<blockquote>\n",
    "<p> 1) Visual inspection. Do simulated observations look like the actual observations.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 2) Tail area probabilities, p-values.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "<p> 3) Quantiles\n",
    "</blockquote>\n",
    "\n",
    "In case you already have some samples of your posterior distribution, e.g. obtained via MCMC, you can readily do posterior predictive tests. This will be a great help to build data models.\n",
    "\n",
    "\n",
    "# Bayesian learning\n",
    "\n",
    "Assume we have observations $d_1,d_2$ then frequent application of Bayes's law yields:\n",
    "\n",
    "<center> $ \\begin{eqnarray} \\pi(y|d_1,d_2,M) &=& \\frac{ \\pi(y|M)\\,\\pi(d_1,d_2|y,M)}{\\pi(d_1,d_2|M)} \\nonumber \\\\ &=&\\frac{\\pi(y|M)\\,\\pi(d_1|y,M)}{\\pi(d_1|M)} \\frac{\\pi(d_2|d_1,y,M)}{\\pi(d_2|d_1,M))} \\nonumber \\\\ &=& \\pi(y|d_1,M) \\frac{\\pi(d_2|d_1,y,M)}{\\pi(d_2|d_1,M))} \\nonumber\\end{eqnarray}$ </center> <br/><br/>\n",
    "\n",
    "Where in the last line we see, that the new posterior $\\pi(y|d_1,d_2,M)$ is just the previous posterior $\\pi(y|d_1,M) $ multiplied with the likelihood of the new observation $d_2$. Bayesian learning can therefore be describes as sequential updating of the posterior distribution in light of new data. The posterior obtained from a previous analysis step acts as a prior in the subsequent analysis step. This is the reasoning behind constructing priors from previously obtained experimental results or available domain knowledge.\n",
    "\n",
    "The results generalize straightforwardly to arbitrary numbers of sequentila observations.\n",
    "\n",
    "\n",
    "# The prior\n",
    "\"It makes no difference what source your priors have, Non extreme priors converge in the limit.\" Bruno de Finetti 1964\n",
    "\n",
    "## Example 1d Gaussian:\n",
    "\n",
    "Assume we obtained $d_1,d_2,...d_N$ independent observations. Let:<br/><br/>\n",
    "\n",
    "\n",
    "<center> $\\begin{eqnarray}\\pi(d_i|y,M) &=& \\frac{\\mathrm{e}^{\\frac{-1}{2} \\frac{(d_i -y)^2}{\\sigma^2_N} }}{\\sqrt{2\\pi\\sigma^2_N }} \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "\n",
    "be a Gaussian likelihood with noise variance $\\sigma^2_N$.\n",
    "\n",
    "Let:<br/><br/>\n",
    "\n",
    "\n",
    "<center> $\\begin{eqnarray}\\pi(y|M) &=& \\frac{\\mathrm{e}^{\\frac{-1}{2} \\frac{(y -\\mu)^2}{\\sigma^2_p} }}{\\sqrt{2\\pi\\sigma^2_p }} \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "\n",
    "be a Gaussian prior distribution with mean $\\mu$ and variance $\\sigma^2_p$.\n",
    "\n",
    "Then:<br/><br/>\n",
    "\n",
    "\n",
    "<center> $\\begin{eqnarray}\\pi(y|d_1,d_2,...,d_N,M) &=& \\pi(y|M) \\prod_i \\pi(d_i|y,M)  \\nonumber \\\\ &=& \\frac{\\mathrm{e}^{\\frac{-1}{2} \\frac{(y -\\mu)^2}{\\sigma^2_p} }}{\\sqrt{2\\pi\\sigma^2_p }} \\prod_i \\frac{\\mathrm{e}^{\\frac{-1}{2} \\frac{(d_i -y)^2}{\\sigma^2_N} }}{\\sqrt{2\\pi\\sigma^2_N }} \\nonumber \\\\ &\\propto& \\mathrm{e}^{\\frac{-1}{2} \\frac{(y -\\mu)^2}{\\sigma^2_p} } \\mathrm{e}^{\\frac{-1}{2} \\sum_i \\frac{(d_i -y)^2}{\\sigma^2_N} } \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "Then the logarithmic posterior distribution, up to a cosntant, is given by:<br/><br/>\n",
    "\n",
    "<center> $\\begin{eqnarray}-2\\mathrm{ln}(\\pi(y|d_1,d_2,...,d_N,M)) &\\sim& \\frac{(y -\\mu)^2}{\\sigma^2_p} + \\sum_i \\frac{(d_i -y)^2}{\\sigma^2_N} \\nonumber \\\\ &\\sim& \\frac{y^2 -2y\\mu +y^2}{\\sigma^2_p} + \\sum_i \\frac{y^2_i -2yd_i +d_i^2}{\\sigma^2_N}  \\nonumber \\\\ &\\sim& y^2 (\\frac{1}{\\sigma^2_p} + \\frac{N}{\\sigma^2_N}) - 2 y \\left(\\frac{\\mu}{\\sigma^2_p} + \\frac{ \\sum_i d_i}{\\sigma^2_N}\\right)  + \\frac{\\mu^2}{\\sigma^2_p} +  \\frac{\\left(\\sum_i d_i\\right)^2}{\\sigma^2_N} \\nonumber \\\\ &\\sim& y^2 (\\frac{1}{\\sigma^2_p} + \\frac{N}{\\sigma^2_N}) - 2 y \\left(\\frac{\\mu}{\\sigma^2_p} + \\frac{ \\sum_i d_i}{\\sigma^2_N} \\right)  + \\tilde{C} \\nonumber \\\\ &\\sim& y^2 (\\frac{1}{\\sigma^2_p} + \\frac{N}{\\sigma^2_N}) - 2 y \\left(\\frac{\\mu}{\\sigma^2_p} + \\frac{ \\sum_i d_i}{\\sigma^2_N}\\right) \\nonumber \\\\ &\\sim& (\\frac{1}{\\sigma^2_p} + \\frac{N}{\\sigma^2_N}) \\left[ y^2 - 2 y \\left( \\frac{\\sigma^2_p\\,\\sigma^2_N}{\\sigma^2_N + N\\,\\sigma^2_p} \\right) \\left(\\frac{\\mu}{\\sigma^2_p} + \\frac{ \\sum_i d_i}{\\sigma^2_N}\\right) \\right]   \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "Let $1/\\sigma^2_p  + N/\\sigma^2_N = 1/\\sigma^2_c $ and $\\left( \\frac{\\sigma^2_p\\,\\sigma^2_N}{\\sigma^2_N + N\\,\\sigma^2_p} \\right) \\left(\\frac{\\mu}{\\sigma^2_p} + \\frac{ \\sum_i d_i}{\\sigma^2_N}\\right) = \\mu_c$ then <br/><br/>\n",
    "\n",
    "\n",
    "<center> $\\begin{eqnarray}-2\\mathrm{ln}(\\pi(y|d_1,d_2,...,d_N,M)) &\\sim& \\frac{1}{\\sigma^2_c}\\left[ y^2 - 2 y \\mu_c + \\mu^2_c -\\mu^2_c \\right] \\nonumber \\\\ &\\sim& \\frac{1}{\\sigma^2_c}\\left[ y - \\mu_c \\right]^2  -\\frac{\\mu^2_c}{\\sigma^2_c} \\nonumber \\\\ &\\sim& \\frac{1}{\\sigma^2_c}\\left[ y - \\mu_c \\right]^2   \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "Therefore the posterior distribution up to a constant normalization is given by:<br/><br/>\n",
    "\n",
    "<center> $\\begin{eqnarray}\\pi(y|d_1,d_2,...,d_N,M) &\\propto& \\mathrm{e}^{\\frac{-1}{2} \\frac{1}{\\sigma^2_c}\\left[ y - \\mu_c \\right]^2 } \\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "By normalizing the distribution we obtain the posterior:<br/><br/>\n",
    "\n",
    "<center> $\\begin{eqnarray}\\pi(y|d_1,d_2,...,d_N,M) &=& \\frac{\\mathrm{e}^{\\frac{-1}{2} \\frac{1}{\\sigma^2_c}\\left[ y - \\mu_c \\right]^2 } }{\\sqrt(2\\pi \\sigma^2_c)}\\end{eqnarray} $ </center> <br/><br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Also note, that in the limit $N \\to \\infty$ we obtain:\n",
    "\n",
    "<center> $\\sigma^2_c = \\frac{\\sigma^2_N}{N}$ and  $\\mu_c = \\frac{\\sum_i d_i}{N}$ </center> <br/><br/>\n",
    "\n",
    "Therefore in the limit non-extreme priors will have no effect.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
